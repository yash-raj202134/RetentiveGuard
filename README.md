
# RetentiveGuard

**RetentiveGuard** is a machine learning model that can accurately detect whether an essay was written by a student or a Large Language Model (LLM). This innovative project leverages natural language processing and deep learning techniques to differentiate between human-written and LLM-generated essays.
![](image.jpeg)
## Features

- **Essay Prompt Analysis**: Detects essays written in response to specific prompts, understanding their content and context.
- **LLM vs. Human Essay Identification**: Accurately distinguishes between essays written by students and essays generated by an LLM.
- **Customizable Inputs**: Accepts essays with or without source texts, reflecting the variety of real-world scenarios.
- **Training and Testing**: Utilizes essays from two prompts for training and the remaining prompts for testing.
- **High Accuracy**: Provides high precision in distinguishing between the two essay sources.

## Dataset

The project uses a unique dataset composed of essays written in response to one of seven essay prompts:

- **Training Set**: Composed mainly of student-written essays with a few generated essays.
- **Test Set**: A hidden set of essays used to evaluate model performance.

## How to Use RetentiveGuard

1. **Clone the Repository**:
    ```bash
    git clone [YOUR_REPOSITORY_URL]
    ```

2. **Install Dependencies**:
    - Use Conda to create and activate an environment:
        ```bash
        conda create -n retentiveguard python=3.8 -y
        conda activate retentiveguard
        ```
    - Install project requirements:
        ```bash
        pip install -r requirements.txt
        ```

3. **Prepare Data**:
    - Ensure the training and test sets are available in the specified directory.
    - The directory structure and file format should follow the guidelines provided in the project documentation.

4. **Train the Model**:
    - Run the training script:
        ```bash
        python train.py
        ```

5. **Evaluate the Model**:
    - Test the model using the hidden test set:
        ```bash
        python evaluate.py
        ```

6. **Use RetentiveGuard**:
    - Apply the model to new essays to determine whether they were written by a student or LLM.

## Contributing

We welcome contributions from the community! If you would like to contribute, please follow these steps:

- Fork the repository.
- Create a new branch with your changes.
- Make a pull request to the main branch.

## License

This project is licensed under the [LICENSE_NAME] license.

## Author

[YOUR NAME]  
[YOUR EMAIL]

## Acknowledgments

- Thanks to [LIST ANY COLLABORATORS] for their support and contributions to this project.
- [ANY OTHER THANKS OR REFERENCES]

